{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "split-aluminum",
      "metadata": {
        "id": "split-aluminum",
        "papermill": {
          "duration": 0.048394,
          "end_time": "2021-04-15T21:06:39.560571",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.512177",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# LAB | Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-turner",
      "metadata": {
        "id": "prospective-turner",
        "papermill": {
          "duration": 0.045573,
          "end_time": "2021-04-15T21:06:39.651272",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.605699",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "This notebook demonstrates how Pinecone helps you build an extractive question-answering application. To build an extractive question-answering system, we need three main components:\n",
        "\n",
        "- A vector index to store and run semantic search\n",
        "- A retriever model for embedding context passages\n",
        "- A reader model to extract answers\n",
        "\n",
        "We will use the SQuAD dataset, which consists of **questions** and **context** paragraphs containing question **answers**. We generate embeddings for the context passages using the retriever, index them in the vector database, and query with semantic search to retrieve the top k most relevant contexts containing potential answers to our question. We then use the reader model to extract the answers from the returned contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oC3GG-dWkZJ6",
      "metadata": {
        "id": "oC3GG-dWkZJ6"
      },
      "source": [
        "Let's get started by installing the packages needed for notebook to run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740a3cf7-830c-4f1d-bf8d-90d18f908a99",
      "metadata": {
        "id": "740a3cf7-830c-4f1d-bf8d-90d18f908a99"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY= os.getenv('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "terminal-export",
      "metadata": {
        "id": "terminal-export",
        "papermill": {
          "duration": 0.044413,
          "end_time": "2021-04-15T21:06:39.741951",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.697538",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expressed-executive",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-15T21:06:39.845309Z",
          "iopub.status.busy": "2021-04-15T21:06:39.842494Z",
          "iopub.status.idle": "2021-04-15T21:08:22.163939Z",
          "shell.execute_reply": "2021-04-15T21:08:22.164616Z"
        },
        "id": "expressed-executive",
        "papermill": {
          "duration": 102.376674,
          "end_time": "2021-04-15T21:08:22.165052",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.788378",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets pinecone-client sentence-transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ad3840",
      "metadata": {
        "id": "29ad3840"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hgIieQukgagu",
      "metadata": {
        "id": "hgIieQukgagu"
      },
      "source": [
        "Now let's load the SQUAD dataset from the HuggingFace Model Hub. We load the dataset into a pandas dataframe and filter the title, question, and context columns, and we drop any duplicate context passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J250IJeh7NIb",
      "metadata": {
        "id": "J250IJeh7NIb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the squad dataset into a pandas dataframe\n",
        "df = load_dataset(\"squad\", split=\"train\").to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FcmeNO97dHDO",
      "metadata": {
        "id": "FcmeNO97dHDO"
      },
      "outputs": [],
      "source": [
        "# select only title and context column\n",
        "df = None\n",
        "# drop rows containing duplicate context passages\n",
        "df = None\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bbcb57",
      "metadata": {
        "id": "57bbcb57"
      },
      "source": [
        "# Initialize Pinecone Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24d904c",
      "metadata": {
        "id": "e24d904c"
      },
      "source": [
        "The Pinecone index stores vector representations of our context passages which we can retrieve using another vector (query vector). We first need to initialize our connection to Pinecone to create our vector index. For this, we need a free [API key](\"https://app.pinecone.io/\"), and then we initialize the connection like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5983bc0e",
      "metadata": {
        "id": "5983bc0e"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-pinecone pinecone-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092d1e71",
      "metadata": {
        "id": "092d1e71"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# connect to pinecone environment\n",
        "pc = Pinecone(\n",
        "    api_key = PINECONE_API_KEY,\n",
        "    environment='us-east-1'  # find next to API key in console\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58028e12",
      "metadata": {
        "id": "58028e12"
      },
      "source": [
        "Now we create a new index called \"question-answering\" â€” we can name the index anything we want. We specify the metric type as \"cosine\" and dimension as 384 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 384-dimension vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3206184",
      "metadata": {
        "id": "b3206184"
      },
      "outputs": [],
      "source": [
        "index_name = None\n",
        "\n",
        "# check if the extractive-question-answering index exists\n",
        "if index_name not in pinecone.list_indexes().names():\n",
        "    # create the index if it does not exist\n",
        "    None\n",
        "# connect to extractive-question-answering index we created\n",
        "index = pinecone.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e84a3e5",
      "metadata": {
        "id": "6e84a3e5"
      },
      "source": [
        "# Initialize Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oZzhGS1Lpj0g",
      "metadata": {
        "id": "oZzhGS1Lpj0g"
      },
      "source": [
        "Next, we need to initialize our retriever. The retriever will mainly do two things:\n",
        "\n",
        "- Generate embeddings for all context passages (context vectors/embeddings)\n",
        "- Generate embeddings for our questions (query vector/embedding)\n",
        "\n",
        "The retriever will generate embeddings in a way that the questions and context passages containing answers to our questions are nearby in the vector space. We can use cosine similarity to calculate the similarity between the query and context embeddings to find the context passages that contain potential answers to our question.\n",
        "\n",
        "We will use a SentenceTransformer model named ``multi-qa-MiniLM-L6-cos-v1`` designed for semantic search and trained on 215M (question, answer) pairs from diverse sources as our retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a85bb3",
      "metadata": {
        "id": "31a85bb3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# set device to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# load the retriever model from huggingface model hub\n",
        "retriever = None #use the 'multi-qa-MiniLM-L6-cos-v1' model from HuggingFace to build the retriever\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aaad0a2",
      "metadata": {
        "id": "8aaad0a2"
      },
      "source": [
        "# Generate Embeddings and Upsert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hgy7AagJtO_p",
      "metadata": {
        "id": "Hgy7AagJtO_p"
      },
      "source": [
        "Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, context passage, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17824ef",
      "metadata": {
        "id": "a17824ef",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# we will use batches of 64\n",
        "batch_size = 64\n",
        "\n",
        "for i in tqdm(range(0, len(df), batch_size)):\n",
        "    # find end of batch\n",
        "    None\n",
        "    # extract batch\n",
        "    None\n",
        "    # generate embeddings for batch\n",
        "    emb = None\n",
        "    # get metadata\n",
        "    meta = None\n",
        "    # create unique IDs\n",
        "    ids = None\n",
        "    # add all to upsert list\n",
        "    to_upsert = None\n",
        "    # upsert/insert these records to pinecone\n",
        "    _ = index.upsert(vectors=to_upsert)\n",
        "\n",
        "# check that we have all vectors in index\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YFyBYafuJ0y0",
      "metadata": {
        "id": "YFyBYafuJ0y0"
      },
      "source": [
        "# Initialize Reader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HgdiLCz5ynOk",
      "metadata": {
        "id": "HgdiLCz5ynOk"
      },
      "source": [
        "We use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model. We load this model into a \"question-answering\" pipeline from HuggingFace transformers and feed it our questions and context passages individually. The model gives a prediction for each context we pass through the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hg9XTDkIJzH_",
      "metadata": {
        "id": "hg9XTDkIJzH_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = 'deepset/electra-base-squad2'\n",
        "# load the reader model into a question-answering pipeline\n",
        "reader = pipeline(tokenizer=model_name, model=model_name, task='question-answering', device=device)\n",
        "reader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14d89d6",
      "metadata": {
        "id": "e14d89d6"
      },
      "source": [
        "Now all the components we need are ready. Let's write some helper functions to execute our queries. The `get_context` function retrieves the context embeddings containing answers to our question from the Pinecone index, and the `extract_answer` function extracts the answers from these context passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lyYaY3QEQiHZ",
      "metadata": {
        "id": "lyYaY3QEQiHZ"
      },
      "outputs": [],
      "source": [
        "# gets context passages from the pinecone index\n",
        "def get_context(question, top_k):\n",
        "    # generate embeddings for the question\n",
        "    xq = None\n",
        "    # search pinecone index for context passage with the answer\n",
        "    xc = None\n",
        "    # extract the context passage from pinecone search result\n",
        "    c = None\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dc9VYOiUQA7B",
      "metadata": {
        "id": "Dc9VYOiUQA7B"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# extracts answer from the context passage\n",
        "def extract_answer(question, context):\n",
        "    results = []\n",
        "    for c in context:\n",
        "        # feed the reader the question and contexts to extract answers\n",
        "        answer = reader(question=question, context=c)\n",
        "        # add the context to answer dict for printing both together\n",
        "        answer[\"context\"] = c\n",
        "        results.append(answer)\n",
        "    # sort the result based on the score from reader model\n",
        "    sorted_result = pprint(sorted(results, key=lambda x: x['score'], reverse=True))\n",
        "    return sorted_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5E3a3dkJ5ZQD",
      "metadata": {
        "id": "5E3a3dkJ5ZQD"
      },
      "outputs": [],
      "source": [
        "question = \"How much oil is Egypt producing in a day?\"\n",
        "context = get_context(question, top_k = 1)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heKNVbWQ_LtC",
      "metadata": {
        "id": "heKNVbWQ_LtC"
      },
      "source": [
        "As we can see, the retiever is working fine and gets us the context passage that contains the answer to our question. Now let's use the reader to extract the exact answer from the context passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DQ4GWdbMSjPl",
      "metadata": {
        "id": "DQ4GWdbMSjPl"
      },
      "outputs": [],
      "source": [
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fMD_ABuDAyhN",
      "metadata": {
        "id": "fMD_ABuDAyhN"
      },
      "source": [
        "The reader model predicted with 99% accuracy the correct answer *691,000 bbl/d* as seen from the context passage. Let's run few more queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_4NRgV4mGWoj",
      "metadata": {
        "id": "_4NRgV4mGWoj"
      },
      "outputs": [],
      "source": [
        "question = \"What are the first names of the men that invented youtube?\"\n",
        "context = get_context(question, top_k=1)\n",
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juXlctWgJgMF",
      "metadata": {
        "id": "juXlctWgJgMF"
      },
      "outputs": [],
      "source": [
        "question = \"What is Albert Eistein famous for?\"\n",
        "context = get_context(question, top_k=1)\n",
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OhCgeny_BVno",
      "metadata": {
        "id": "OhCgeny_BVno"
      },
      "source": [
        "Let's run another question. This time for top 3 context passages from the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iXACn71xmett",
      "metadata": {
        "id": "iXACn71xmett"
      },
      "outputs": [],
      "source": [
        "question = \"Who was the first person to step foot on the moon?\"\n",
        "context = get_context(question, top_k=3)\n",
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9oCWHy0tPpV",
      "metadata": {
        "id": "c9oCWHy0tPpV"
      },
      "source": [
        "The result looks pretty good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83f9f55-b099-4280-a12e-1d0192f4f5aa",
      "metadata": {
        "id": "d83f9f55-b099-4280-a12e-1d0192f4f5aa"
      },
      "outputs": [],
      "source": [
        "pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fecadc-6976-43b2-90e1-788a8633ecc7",
      "metadata": {
        "id": "27fecadc-6976-43b2-90e1-788a8633ecc7"
      },
      "source": [
        "### Add a few more questions. What did you observe?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "question_answering.ipynb",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-3.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 333.240754,
      "end_time": "2021-04-15T21:12:11.363566",
      "environment_variables": {},
      "exception": null,
      "input_path": "/notebooks/question_answering/question_answering.ipynb",
      "output_path": "/notebooks/tmp/question_answering/question_answering.ipynb",
      "parameters": {},
      "start_time": "2021-04-15T21:06:38.122812",
      "version": "2.3.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "5fe10bf018ef3e697f9035d60bf60847932a12bface18908407fd371fe880db9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}